#!/usr/bin/env python3
"""
GPT-OSS-20B LoRA Fine-tuning Script
====================================

ä½¿ç”¨ Unsloth åœ¨æ‚¨çš„è³‡æ–™é›†ä¸Šé€²è¡Œ LoRA fine-tuning

ä½¿ç”¨æ–¹æ³•:
    python train_gpt_oss_20b.py

åœ¨ tmux èƒŒæ™¯åŸ·è¡Œï¼ˆå»ºè­°ï¼‰:
    tmux new -s finetune
    python train_gpt_oss_20b.py
    # æŒ‰ Ctrl+B ç„¶å¾ŒæŒ‰ D ä¾† detach

    # é‡æ–°é€£æ¥:
    tmux attach -t finetune

æ³¨æ„äº‹é …:
    - è«‹å°‡è¨“ç·´è³‡æ–™æ”¾åœ¨ ./data/YourDataset.csv
    - è³‡æ–™æ ¼å¼ï¼šCSV with 'input' and 'output' columns
    - è¨“ç·´æ™‚é–“è¦–è³‡æ–™é‡å’Œ GPU è€Œå®šï¼Œå»ºè­°ä½¿ç”¨ tmux èƒŒæ™¯åŸ·è¡Œ
"""

import torch
from datasets import load_dataset
from unsloth import FastLanguageModel, is_bfloat16_supported
from unsloth.chat_templates import standardize_sharegpt
from trl import SFTConfig, SFTTrainer

print("=" * 80)
print("ğŸ¦¥ GPT-OSS-20B LoRA Fine-tuning Script")
print("=" * 80)

# ============================================================================
# æ¨¡å‹è¨­å®š
# ============================================================================
MODEL_NAME = "unsloth/gpt-oss-20b-BF16"
MAX_SEQ_LENGTH = 1024
DTYPE = None  # None for auto detection
LOAD_IN_4BIT = False
FULL_FINETUNING = False

# ============================================================================
# LoRA è¨­å®š
# ============================================================================
LORA_R = 32
LORA_ALPHA = 32
LORA_DROPOUT = 0
LORA_BIAS = "none"
LORA_TARGET_MODULES = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]
USE_GRADIENT_CHECKPOINTING = "unsloth"
LORA_RANDOM_STATE = 3407
USE_RSLORA = False
LOFTQ_CONFIG = None

# ============================================================================
# System Prompt
# ============================================================================
SYSTEM_PROMPT = (
    "You are a helpful assistant specialized in technical support.\n"
    "Knowledge cutoff: 2024-06"
)

# ============================================================================
# è¨“ç·´è¨­å®š
# ============================================================================
PER_DEVICE_TRAIN_BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = 4
WARMUP_STEPS = 5
MAX_STEPS = -1  # -1 è¡¨ç¤ºç”± num_train_epochs æ§åˆ¶
NUM_TRAIN_EPOCHS = 300
LEARNING_RATE = 2e-4
OPTIM = "adamw_8bit"
WEIGHT_DECAY = 0.01
LR_SCHEDULER_TYPE = "linear"
TRAINING_SEED = 3407
OUTPUT_DIR = "outputs"
REPORT_TO = "none"

# è©•ä¼°è¨­å®š
EVAL_STRATEGY = "epoch"

# ============================================================================
# è³‡æ–™é›†è¨­å®š
# ============================================================================
CSV_FILE = "./data/YourDataset.csv"  # è«‹å°‡æ‚¨çš„è³‡æ–™é›†å‘½åç‚º YourDataset.csv
TEST_SIZE = 110  # Test å’Œ Validation å„ 110 ç­†ï¼ˆå¯ä¾å¯¦éš›è³‡æ–™é‡èª¿æ•´ï¼‰
RANDOM_SEED = 42

# ============================================================================
# 1. è¼‰å…¥åŸºç¤æ¨¡å‹
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ“¦ è¼‰å…¥åŸºç¤æ¨¡å‹...")
print("=" * 80)

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_NAME,
    dtype=DTYPE,
    max_seq_length=MAX_SEQ_LENGTH,
    load_in_4bit=LOAD_IN_4BIT,
    full_finetuning=FULL_FINETUNING,
)

print(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹: {MODEL_NAME}")

# ============================================================================
# 2. é…ç½® LoRA
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ”§ é…ç½® LoRA...")
print("=" * 80)

model = FastLanguageModel.get_peft_model(
    model,
    r=LORA_R,
    target_modules=LORA_TARGET_MODULES,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    bias=LORA_BIAS,
    use_gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,
    random_state=LORA_RANDOM_STATE,
    use_rslora=USE_RSLORA,
    loftq_config=LOFTQ_CONFIG,
)

print(f"âœ… LoRA é…ç½®å®Œæˆ (r={LORA_R}, alpha={LORA_ALPHA})")

# ============================================================================
# 3. è³‡æ–™è™•ç†å‡½æ•¸
# ============================================================================

def convert_to_sharegpt(example):
    """å°‡ CSV çš„ input/output è½‰æ›ç‚º ShareGPT æ ¼å¼"""
    conversations = [
        {"from": "human", "value": example["input"]},
        {"from": "gpt", "value": example["output"]}
    ]
    return {"conversations": conversations}


def formatting_prompts_func(examples):
    """æ‡‰ç”¨ chat template"""
    convos = examples["conversations"]
    texts = [
        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)
        for convo in convos
    ]
    return {"text": texts}


def prepare_dataset(csv_file):
    """æº–å‚™è³‡æ–™é›†"""
    # æ­¥é©Ÿ 1: Read CSV
    print(f"ğŸ“‚ è®€å– CSV: {csv_file}...")
    dataset = load_dataset("csv", data_files=csv_file, split="train")
    print(f"âœ… è¼‰å…¥äº† {len(dataset)} ç­†è³‡æ–™")
    print(f"ç¬¬ä¸€ç­†è³‡æ–™: {dataset[0]}")

    # æ­¥é©Ÿ 2: è½‰æ›ç‚º ShareGPT æ ¼å¼
    print("ğŸ”„ è½‰æ›ç‚º ShareGPT æ ¼å¼...")
    dataset = dataset.map(convert_to_sharegpt, remove_columns=dataset.column_names)
    print(f"ç¬¬ä¸€ç­†è³‡æ–™: {dataset[0]}")

    # æ­¥é©Ÿ 3: æ¨™æº–åŒ– ShareGPT æ ¼å¼
    print("ğŸ”§ æ¨™æº–åŒ– ShareGPT...")
    dataset = standardize_sharegpt(dataset)
    print(f"ç¬¬ä¸€ç­†è³‡æ–™: {dataset[0]}")

    # æ­¥é©Ÿ 4: æ‡‰ç”¨ chat template è½‰ç‚ºè¨“ç·´æ–‡å­—
    print("ğŸ“ æ‡‰ç”¨ Chat Template...")
    dataset = dataset.map(formatting_prompts_func, batched=True)

    # é è¦½ç¬¬ä¸€ç­†è³‡æ–™
    print("\nğŸ“‹ é è¦½ç¬¬ä¸€ç­†è™•ç†å¾Œçš„è³‡æ–™:")
    print("=" * 80)
    print(dataset[0]["text"][:500])  # åªé¡¯ç¤ºå‰ 500 å­—å…ƒ
    print("..." if len(dataset[0]["text"]) > 500 else "")
    print("=" * 80)

    return dataset


# ============================================================================
# 4. è¼‰å…¥ä¸¦åˆ†å‰²è³‡æ–™é›†
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ“Š è¼‰å…¥ä¸¦åˆ†å‰²è³‡æ–™é›†...")
print("=" * 80)

# è™•ç†è³‡æ–™é›†
processed_dataset = prepare_dataset(CSV_FILE)
print(f"\nğŸ“Š åŸå§‹è³‡æ–™é›†å¤§å°: {len(processed_dataset)} ç­†")

# è³‡æ–™é›†åˆ†å‰²ï¼šTrain / Validation / Test
print("\n" + "=" * 80)
print("âœ‚ï¸  é–‹å§‹åˆ†å‰²è³‡æ–™é›†...")
print("=" * 80)

# å…ˆåˆ‡å‡º test set (110 ç­†)
split_1 = processed_dataset.train_test_split(test_size=TEST_SIZE, seed=RANDOM_SEED)
train_val_dataset = split_1['train']
test_dataset = split_1['test']

# å†å¾å‰©ä¸‹çš„è³‡æ–™ä¸­åˆ‡å‡º validation set (110 ç­†)
split_2 = train_val_dataset.train_test_split(test_size=TEST_SIZE, seed=RANDOM_SEED)
train_dataset = split_2['train']
val_dataset = split_2['test']

print(f"âœ… Train è³‡æ–™é›†: {len(train_dataset)} ç­†")
print(f"âœ… Validation è³‡æ–™é›†: {len(val_dataset)} ç­†")
print(f"âœ… Test è³‡æ–™é›†: {len(test_dataset)} ç­†")
print(f"ğŸ“Š ç¸½è¨ˆ: {len(train_dataset) + len(val_dataset) + len(test_dataset)} ç­†")

# å„²å­˜åˆ†å‰²å¾Œçš„è³‡æ–™é›†
output_path_train = "processed_dataset_gpt_oss/train"
output_path_val = "processed_dataset_gpt_oss/validation"
output_path_test = "processed_dataset_gpt_oss/test"

train_dataset.save_to_disk(output_path_train)
val_dataset.save_to_disk(output_path_val)
test_dataset.save_to_disk(output_path_test)

print(f"\nğŸ’¾ Train è³‡æ–™é›†å·²å„²å­˜è‡³: {output_path_train}")
print(f"ğŸ’¾ Validation è³‡æ–™é›†å·²å„²å­˜è‡³: {output_path_val}")
print(f"ğŸ’¾ Test è³‡æ–™é›†å·²å„²å­˜è‡³: {output_path_test}")

# ============================================================================
# 5. åˆå§‹åŒ–è¨“ç·´å™¨
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ‹ï¸  åˆå§‹åŒ–è¨“ç·´å™¨...")
print("=" * 80)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=processed_dataset,
    eval_dataset=val_dataset,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LENGTH,
    dataset_num_proc=2,
    packing=False,
    args=SFTConfig(
        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        warmup_steps=WARMUP_STEPS,
        max_steps=MAX_STEPS,
        num_train_epochs=NUM_TRAIN_EPOCHS,
        learning_rate=LEARNING_RATE,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim=OPTIM,
        weight_decay=WEIGHT_DECAY,
        lr_scheduler_type=LR_SCHEDULER_TYPE,
        seed=TRAINING_SEED,
        output_dir=OUTPUT_DIR,
        report_to=REPORT_TO,
        # è©•ä¼°è¨­å®š
        eval_strategy=EVAL_STRATEGY,
    ),
)

print("âœ… è¨“ç·´å™¨åˆå§‹åŒ–å®Œæˆ")

# ============================================================================
# 6. é–‹å§‹è¨“ç·´
# ============================================================================
print("\n" + "=" * 80)
print("ğŸš€ é–‹å§‹è¨“ç·´...")
print(f"ğŸ“Š Train: {len(processed_dataset)} ç­† | Validation: {len(val_dataset)} ç­†")
print("=" * 80)

trainer_stats = trainer.train()

print("\n" + "=" * 80)
print("âœ… è¨“ç·´å®Œæˆ!")
print("=" * 80)

# ============================================================================
# 7. å„²å­˜æ¨¡å‹
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ’¾ å„²å­˜æ¨¡å‹...")
print("=" * 80)

model.save_pretrained("RANK32_gpt_oss_finetuned")
tokenizer.save_pretrained("RANK32_gpt_oss_finetuned")

print("âœ… æ¨¡å‹å·²å„²å­˜è‡³: ./RANK32_gpt_oss_finetuned")

# ============================================================================
# 8. æ¸¬è©¦æ¨è«–
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ§ª æ¸¬è©¦æ¨è«–...")
print("=" * 80)

# æº–å‚™æ¸¬è©¦è¨Šæ¯
messages = [
    {"role": "user", "content": "What is the purpose of SEMI E88?"},
]

inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
    return_dict=True,
    reasoning_effort="medium",
).to(model.device)

from transformers import TextStreamer

print("\nğŸ“ ç”Ÿæˆå›ç­”:")
print("-" * 80)
_ = model.generate(**inputs, max_new_tokens=512, streamer=TextStreamer(tokenizer))
print("-" * 80)

print("\n" + "=" * 80)
print("âœ… å…¨éƒ¨å®Œæˆ!")
print("=" * 80)
print(f"ğŸ“ æ¨¡å‹ä½ç½®: ./RANK32_gpt_oss_finetuned")
print(f"ğŸ“ è¼¸å‡ºä½ç½®: ./{OUTPUT_DIR}")
print(f"ğŸ“ è³‡æ–™é›†ä½ç½®: ./processed_dataset_gpt_oss/")
