#!/usr/bin/env python3
"""
GPT-OSS-20B Fine-Tuning Evaluation Script (Complete Version)
===========================================================

å®Œæ•´è©•æ¸¬è…³æœ¬,JSON å ±å‘ŠåŒ…å«:
1. Fine-Tune åƒæ•¸æ•¸é‡
2. Training Loss
3. Validation Loss
4. BLEU
5. ROUGE
6. METEOR
7. Perplexity

ä½¿ç”¨æ–¹æ³•:
    python evaluation.py --adapter_path ./checkpoints --test_data ./data/YourDataset.csv

åœ¨ tmux èƒŒæ™¯åŸ·è¡Œ:
    tmux new -s eval
    python evaluation.py --adapter_path ./checkpoints --test_data ./data/YourDataset.csv
    # æŒ‰ Ctrl+B ç„¶å¾ŒæŒ‰ D ä¾† detach
"""

import argparse
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import torch
from datasets import load_dataset
from tqdm import tqdm


# ============================================================================
# è¼”åŠ©å‡½æ•¸
# ============================================================================

def check_dependencies():
    """æª¢æŸ¥å¿…è¦çš„ä¾è³´"""
    print("=" * 80)
    print("ğŸ“¦ æª¢æŸ¥ä¾è³´å¥—ä»¶...")
    print("=" * 80)

    required = ['unsloth', 'evaluate', 'nltk', 'datasets', 'torch', 'tqdm', 'pandas']
    missing = []

    for pkg in required:
        try:
            __import__(pkg)
            print(f"âœ… {pkg}")
        except ImportError:
            print(f"âŒ {pkg}")
            missing.append(pkg)

    if missing:
        print(f"\nâš ï¸  ç¼ºå°‘å¥—ä»¶: {', '.join(missing)}")
        print(f"è«‹åŸ·è¡Œ: pip install {' '.join(missing)}")
        return False

    print("âœ… æ‰€æœ‰ä¾è³´å·²å°±ç·’\n")
    return True


def setup_nltk():
    """è¨­ç½® NLTK è³‡æ–™"""
    import nltk
    print("ğŸ“¥ ä¸‹è¼‰ NLTK è³‡æ–™...")
    try:
        nltk.download('wordnet', quiet=True)
        nltk.download('omw-1.4', quiet=True)
        nltk.download('punkt', quiet=True)
        print("âœ… NLTK è³‡æ–™ä¸‹è¼‰å®Œæˆ\n")
    except Exception as e:
        print(f"âš ï¸  NLTK ä¸‹è¼‰å¤±æ•—: {e}\n")


def count_parameters(model):
    """
    è¨ˆç®—æ¨¡å‹åƒæ•¸æ•¸é‡

    Returns:
        dict: åŒ…å« total, trainable, percentage çš„å­—å…¸
    """
    total_params = 0
    trainable_params = 0

    for param in model.parameters():
        num = param.numel()
        total_params += num
        if param.requires_grad:
            trainable_params += num

    percentage = 100 * trainable_params / total_params if total_params > 0 else 0

    return {
        'total_parameters': total_params,
        'trainable_parameters': trainable_params,
        'trainable_percentage': round(percentage, 4)
    }


# ============================================================================
# æ¨¡å‹è¼‰å…¥
# ============================================================================

def load_model(adapter_path, max_seq_length=1024, load_in_4bit=False):
    """
    è¼‰å…¥æ¨¡å‹å’Œ adapter

    Returns:
        model, tokenizer, adapter_loaded, param_info
    """
    from unsloth import FastLanguageModel

    print("=" * 80)
    print("ğŸ”„ è¼‰å…¥æ¨¡å‹")
    print("=" * 80)

    # è¼‰å…¥åŸºç¤æ¨¡å‹
    print("\næ­¥é©Ÿ 1/3: è¼‰å…¥åŸºç¤æ¨¡å‹ (unsloth/gpt-oss-20b-BF16)...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/gpt-oss-20b-BF16",
        dtype=None,
        max_seq_length=max_seq_length,
        load_in_4bit=load_in_4bit,
    )
    print("âœ… åŸºç¤æ¨¡å‹è¼‰å…¥å®Œæˆ")

    # è¼‰å…¥ Adapter
    print(f"\næ­¥é©Ÿ 2/3: è¼‰å…¥ Adapter ({adapter_path})...")
    adapter_loaded = False

    if os.path.exists(adapter_path):
        try:
            model.load_adapter(adapter_path, adapter_name="finetuned")
            print("âœ… æˆåŠŸè¼‰å…¥ adapter")
            adapter_loaded = True
        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥ adapter å¤±æ•—: {e}")
            print("âš ï¸  å°‡ä½¿ç”¨åŸºç¤æ¨¡å‹é€²è¡Œè©•æ¸¬")
    else:
        print(f"âš ï¸  Adapter è·¯å¾‘ä¸å­˜åœ¨: {adapter_path}")

    # è¨­ç½®æ¨ç†æ¨¡å¼
    print("\næ­¥é©Ÿ 3/3: è¨­ç½®æ¨ç†æ¨¡å¼...")
    FastLanguageModel.for_inference(model)
    print("âœ… æ¨¡å‹å°±ç·’!")

    # è¨ˆç®—åƒæ•¸
    print("\nğŸ“Š è¨ˆç®—æ¨¡å‹åƒæ•¸...")
    param_info = count_parameters(model)
    print(f"   ç¸½åƒæ•¸: {param_info['total_parameters']:,}")
    print(f"   å¯è¨“ç·´åƒæ•¸: {param_info['trainable_parameters']:,}")
    print(f"   å¯è¨“ç·´æ¯”ä¾‹: {param_info['trainable_percentage']:.2f}%")
    print()

    return model, tokenizer, adapter_loaded, param_info


def load_data(csv_file, test_size=0.2, seed=42):
    """è¼‰å…¥æ¸¬è©¦è³‡æ–™"""
    print("=" * 80)
    print("ğŸ“‚ è¼‰å…¥æ¸¬è©¦è³‡æ–™")
    print("=" * 80)

    if not os.path.exists(csv_file):
        raise FileNotFoundError(f"æª”æ¡ˆä¸å­˜åœ¨: {csv_file}")

    print(f"\nå¾ CSV è¼‰å…¥: {csv_file}")
    dataset = load_dataset("csv", data_files=csv_file, split="train")

    print(f"åˆ†å‰²è³‡æ–™é›† (æ¸¬è©¦é›†: {test_size:.0%})")
    split = dataset.train_test_split(test_size=test_size, seed=seed)
    test_dataset = split["test"]

    print(f"âœ… è¼‰å…¥äº† {len(test_dataset)} ç­†æ¸¬è©¦è³‡æ–™\n")
    return test_dataset


# ============================================================================
# æ€§èƒ½è©•ä¼°
# ============================================================================

class PerformanceTracker:
    """è¿½è¹¤æ€§èƒ½æŒ‡æ¨™"""

    def __init__(self):
        self.start_time = None
        self.times = []
        self.memories = []
        self.training_losses = []
        self.validation_losses = []

    def start(self):
        """é–‹å§‹è¨ˆæ™‚"""
        self.start_time = time.time()
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()

    def end(self):
        """çµæŸè¨ˆæ™‚"""
        if self.start_time:
            elapsed = time.time() - self.start_time
            self.times.append(elapsed)
            return elapsed
        return None

    def record_memory(self):
        """è¨˜éŒ„ GPU è¨˜æ†¶é«”"""
        if torch.cuda.is_available():
            mem_gb = torch.cuda.max_memory_allocated() / 1024**3
            self.memories.append(mem_gb)
            return mem_gb
        return 0

    def get_summary(self):
        """ç²å–æ‘˜è¦"""
        return {
            'total_inference_time_seconds': sum(self.times),
            'peak_gpu_memory_gb': max(self.memories) if self.memories else 0,
        }


def run_performance_eval(model, tokenizer, dataset, sample_size=50):
    """åŸ·è¡Œæ€§èƒ½è©•ä¼°"""
    print("=" * 80)
    print("ğŸ“Š æ€§èƒ½è©•ä¼°")
    print("=" * 80)

    tracker = PerformanceTracker()
    tracker.start()

    sample_size = min(sample_size, len(dataset))
    print(f"\nè©•ä¼° {sample_size} å€‹æ¨£æœ¬...\n")

    for idx in tqdm(range(sample_size), desc="æ€§èƒ½æ¸¬è©¦"):
        if idx % 10 == 0:
            tracker.record_memory()

        messages = [{"role": "user", "content": dataset[idx]["input"]}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
            reasoning_effort="medium",
        ).to(model.device)

        with torch.no_grad():
            model.generate(**inputs, max_new_tokens=512)

    elapsed = tracker.end()
    tracker.record_memory()

    print(f"\nâœ… æ€§èƒ½è©•ä¼°å®Œæˆ")
    print(f"   ç¸½æ™‚é–“: {elapsed:.2f} ç§’")
    print(f"   å¹³å‡æ¯æ¨£æœ¬: {elapsed/sample_size:.2f} ç§’\n")

    return tracker


# ============================================================================
# å“è³ªè©•ä¼°
# ============================================================================

class QualityEvaluator:
    """å“è³ªæŒ‡æ¨™è©•ä¼°å™¨"""

    def __init__(self):
        import evaluate
        print("è¼‰å…¥è©•ä¼°æŒ‡æ¨™...")
        self.bleu = evaluate.load("bleu")
        self.rouge = evaluate.load("rouge")
        self.meteor = evaluate.load("meteor")
        print("âœ… æŒ‡æ¨™è¼‰å…¥å®Œæˆ\n")

    def compute_metrics(self, predictions, references):
        """è¨ˆç®—æ‰€æœ‰å“è³ªæŒ‡æ¨™"""
        results = {}

        # BLEU
        print("è¨ˆç®— BLEU...")
        bleu = self.bleu.compute(
            predictions=predictions,
            references=[[r] for r in references]
        )
        results['bleu'] = bleu['bleu']

        # ROUGE
        print("è¨ˆç®— ROUGE...")
        rouge = self.rouge.compute(predictions=predictions, references=references)
        results['rouge1'] = rouge['rouge1']
        results['rouge2'] = rouge['rouge2']
        results['rougeL'] = rouge['rougeL']

        # METEOR
        print("è¨ˆç®— METEOR...")
        meteor = self.meteor.compute(predictions=predictions, references=references)
        results['meteor'] = meteor['meteor']

        return results

    def compute_perplexity(self, model, tokenizer, texts, max_samples=20):
        """è¨ˆç®— Perplexity - Manual loss computation to handle BFloat16"""
        print(f"è¨ˆç®— Perplexity (å‰ {max_samples} å€‹æ¨£æœ¬)...")

        total_nll = 0
        total_tokens = 0

        # Ensure model is in eval mode
        model.eval()

        for idx, text in enumerate(tqdm(texts[:max_samples], desc="Perplexity")):
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}

            with torch.no_grad():
                try:
                    # Get logits without computing loss (avoids BFloat16 bmm issue)
                    outputs = model(**inputs)
                    logits = outputs.logits

                    # Manually compute cross-entropy loss
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = inputs["input_ids"][..., 1:].contiguous()

                    # Convert to float32 for loss computation
                    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
                    loss = loss_fct(
                        shift_logits.float().view(-1, shift_logits.size(-1)),
                        shift_labels.view(-1)
                    )

                    nll = loss.sum().item()
                    tokens = shift_labels.numel()

                    total_nll += nll
                    total_tokens += tokens

                    if idx % 5 == 0:
                        print(f"[DEBUG] Sample {idx}: loss={nll/tokens:.4f}, tokens={tokens}")

                except Exception as e:
                    print(f"\nâš ï¸  è·³éæ¨£æœ¬ {idx} (éŒ¯èª¤: {e})")
                    continue

        if total_tokens == 0:
            print("âš ï¸  ç„¡æ³•è¨ˆç®— Perplexity,è¿”å› None")
            return None

        avg_nll = total_nll / total_tokens
        perplexity = np.exp(avg_nll)
        print(f"\nâœ… Perplexity = {perplexity:.4f}")

        return perplexity


def extract_response(text):
    """æå– assistant å›ç­”"""
    if "<|start|>assistant<|message|>" in text:
        text = text.split("<|start|>assistant<|message|>")[-1]
        text = text.split("<|return|>")[0].strip()
    return text


def run_quality_eval(model, tokenizer, dataset, eval_size=100):
    """åŸ·è¡Œå“è³ªè©•ä¼°"""
    print("=" * 80)
    print("ğŸ¯ å“è³ªè©•ä¼°")
    print("=" * 80)

    eval_size = min(eval_size, len(dataset))
    print(f"\nè©•ä¼° {eval_size} å€‹æ¨£æœ¬...\n")

    evaluator = QualityEvaluator()

    predictions = []
    references = []

    # ç”Ÿæˆé æ¸¬
    print("ç”Ÿæˆé æ¸¬...")
    for idx in tqdm(range(eval_size), desc="ç”Ÿæˆ"):
        example = dataset[idx]

        messages = [{"role": "user", "content": example["input"]}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
            reasoning_effort="medium",
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=512, do_sample=False)

        text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        text = extract_response(text)

        predictions.append(text)
        references.append(example["output"])

    # è¨ˆç®—æŒ‡æ¨™
    print("\n" + "=" * 60)
    print("è¨ˆç®—å“è³ªæŒ‡æ¨™...")
    print("=" * 60 + "\n")

    results = evaluator.compute_metrics(predictions, references)
    perplexity = evaluator.compute_perplexity(model, tokenizer, references, max_samples=20)
    results['perplexity'] = perplexity if perplexity is not None else 0.0

    # é¡¯ç¤ºçµæœ
    print("\n" + "=" * 60)
    print("ğŸ“ˆ å“è³ªæŒ‡æ¨™:")
    print("=" * 60)
    for k, v in results.items():
        print(f"   {k:15s}: {v:.4f}")
    print()

    return results, predictions, references


# ============================================================================
# å ±å‘Šç”Ÿæˆ
# ============================================================================

def generate_report(
    param_info,
    perf_tracker,
    quality_results,
    predictions,
    references,
    adapter_path,
    adapter_loaded,
    output_dir="./evaluation_results"
):
    """ç”Ÿæˆè©•ä¼°å ±å‘Š"""
    print("=" * 80)
    print("ğŸ“‹ ç”Ÿæˆè©•ä¼°å ±å‘Š")
    print("=" * 80)

    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # å»ºç«‹å®Œæ•´çš„ JSON å ±å‘Š
    report = {
        # åŸºæœ¬è³‡è¨Š
        "evaluation_info": {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "model_name": "gpt-oss-20b-BF16",
            "adapter_path": adapter_path,
            "adapter_loaded": adapter_loaded,
            "test_samples": len(predictions),
        },

        # æ¨¡å‹åƒæ•¸ (åŒ…å« Fine-Tune åƒæ•¸æ•¸é‡)
        "model_parameters": {
            "total_parameters": param_info['total_parameters'],
            "trainable_parameters": param_info['trainable_parameters'],  # Fine-Tune åƒæ•¸æ•¸é‡
            "trainable_percentage": param_info['trainable_percentage'],
        },

        # æ€§èƒ½æŒ‡æ¨™
        "performance_metrics": perf_tracker.get_summary(),

        # Training & Validation Loss
        "training_loss": {
            "average": sum(perf_tracker.training_losses) / len(perf_tracker.training_losses) if perf_tracker.training_losses else None,
            "final": perf_tracker.training_losses[-1] if perf_tracker.training_losses else None,
            "history": perf_tracker.training_losses if perf_tracker.training_losses else [],
        },
        "validation_loss": {
            "average": sum(perf_tracker.validation_losses) / len(perf_tracker.validation_losses) if perf_tracker.validation_losses else None,
            "final": perf_tracker.validation_losses[-1] if perf_tracker.validation_losses else None,
            "history": perf_tracker.validation_losses if perf_tracker.validation_losses else [],
        },

        # å“è³ªæŒ‡æ¨™ (BLEU, ROUGE, METEOR, Perplexity)
        "quality_metrics": {
            "bleu": quality_results.get('bleu', 0),
            "rouge1": quality_results.get('rouge1', 0),
            "rouge2": quality_results.get('rouge2', 0),
            "rougeL": quality_results.get('rougeL', 0),
            "meteor": quality_results.get('meteor', 0),
            "perplexity": quality_results.get('perplexity', 0),
        },

        # å¯¦éš›é æ¸¬èˆ‡åƒè€ƒç­”æ¡ˆå°ç…§
        "predictions_vs_references": [
            {
                "index": idx,
                "prediction": pred,
                "reference": ref
            }
            for idx, (pred, ref) in enumerate(zip(predictions, references))
        ],
    }

    # å„²å­˜ JSON å ±å‘Š
    json_file = os.path.join(output_dir, f"evaluation_report_{timestamp}.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)

    print(f"\nğŸ’¾ JSON å ±å‘Šå·²å„²å­˜: {json_file}")
    print("\nâœ… JSON å ±å‘ŠåŒ…å«ä»¥ä¸‹æŒ‡æ¨™:")
    print("   1. Fine-Tune åƒæ•¸æ•¸é‡: model_parameters.trainable_parameters")
    print("   2. Training Loss: training_loss.average, training_loss.final")
    print("   3. Validation Loss: validation_loss.average, validation_loss.final")
    print("   4. BLEU: quality_metrics.bleu")
    print("   5. ROUGE: quality_metrics.rouge1, rouge2, rougeL")
    print("   6. METEOR: quality_metrics.meteor")
    print("   7. Perplexity: quality_metrics.perplexity")
    print("   8. é æ¸¬èˆ‡åƒè€ƒå°ç…§: predictions_vs_references (æ¯ç­†åŒ…å« index, prediction, reference)")

    # ä¹Ÿå„²å­˜ Excel ç‰ˆæœ¬ (æ‰å¹³åŒ–é¡¯ç¤º)
    flat_report = {}
    flat_report['è©•ä¼°æ™‚é–“'] = report['evaluation_info']['timestamp']
    flat_report['æ¨¡å‹'] = report['evaluation_info']['model_name']
    flat_report['Adapterå·²è¼‰å…¥'] = 'æ˜¯' if report['evaluation_info']['adapter_loaded'] else 'å¦'
    flat_report['æ¸¬è©¦æ¨£æœ¬æ•¸'] = report['evaluation_info']['test_samples']

    # åƒæ•¸
    for k, v in report['model_parameters'].items():
        flat_report[f'[åƒæ•¸] {k}'] = v

    # æ€§èƒ½
    for k, v in report['performance_metrics'].items():
        flat_report[f'[æ€§èƒ½] {k}'] = v

    # Loss
    if report['training_loss']['average'] is not None:
        flat_report['[Loss] Training (avg)'] = report['training_loss']['average']
        flat_report['[Loss] Training (final)'] = report['training_loss']['final']
    if report['validation_loss']['average'] is not None:
        flat_report['[Loss] Validation (avg)'] = report['validation_loss']['average']
        flat_report['[Loss] Validation (final)'] = report['validation_loss']['final']

    # å“è³ª
    for k, v in report['quality_metrics'].items():
        flat_report[f'[å“è³ª] {k}'] = v

    df = pd.DataFrame([flat_report]).T
    df.columns = ['æ•¸å€¼']

    excel_file = os.path.join(output_dir, f"evaluation_report_{timestamp}.xlsx")
    df.to_excel(excel_file)
    print(f"ğŸ’¾ Excel å ±å‘Šå·²å„²å­˜: {excel_file}")

    # é¡¯ç¤ºå ±å‘Š
    print("\n" + "=" * 80)
    print("ğŸ“Š è©•ä¼°å ±å‘Šæ‘˜è¦")
    print("=" * 80)
    print(df.to_string())
    print()

    return json_file


# ============================================================================
# ä¸»ç¨‹å¼
# ============================================================================

def parse_args():
    """è§£æå‘½ä»¤åˆ—åƒæ•¸"""
    parser = argparse.ArgumentParser(
        description='GPT-OSS-20B Complete Evaluation Pipeline',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('--adapter_path', type=str, required=True,
                        help='Adapter è·¯å¾‘')
    parser.add_argument('--test_data', type=str, required=True,
                        help='æ¸¬è©¦è³‡æ–™ CSV æª”æ¡ˆ')
    parser.add_argument('--output_dir', type=str, default='./evaluation_results',
                        help='è¼¸å‡ºç›®éŒ„ (é è¨­: ./evaluation_results)')
    parser.add_argument('--max_seq_length', type=int, default=1024,
                        help='æœ€å¤§åºåˆ—é•·åº¦ (é è¨­: 1024)')
    parser.add_argument('--load_in_4bit', action='store_true',
                        help='ä½¿ç”¨ 4-bit é‡åŒ–')
    parser.add_argument('--test_size', type=float, default=0.2,
                        help='æ¸¬è©¦é›†æ¯”ä¾‹ (é è¨­: 0.2)')
    parser.add_argument('--perf_samples', type=int, default=50,
                        help='æ€§èƒ½è©•ä¼°æ¨£æœ¬æ•¸ (é è¨­: 50)')
    parser.add_argument('--eval_samples', type=int, default=100,
                        help='å“è³ªè©•ä¼°æ¨£æœ¬æ•¸ (é è¨­: 100)')

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•¸"""
    args = parse_args()

    print("\n" + "=" * 80)
    print("ğŸš€ GPT-OSS-20B Complete Evaluation Pipeline")
    print("=" * 80)
    print(f"\né–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Adapter: {args.adapter_path}")
    print(f"æ¸¬è©¦è³‡æ–™: {args.test_data}")
    print()

    try:
        # 1. æª¢æŸ¥ä¾è³´
        if not check_dependencies():
            sys.exit(1)
        setup_nltk()

        # 2. è¼‰å…¥æ¨¡å‹
        model, tokenizer, adapter_loaded, param_info = load_model(
            args.adapter_path,
            args.max_seq_length,
            args.load_in_4bit
        )

        # 3. è¼‰å…¥è³‡æ–™
        dataset = load_data(args.test_data, args.test_size)

        # 4. æ€§èƒ½è©•ä¼°
        perf_tracker = run_performance_eval(
            model, tokenizer, dataset, args.perf_samples
        )

        # 5. å“è³ªè©•ä¼°
        quality_results, predictions, references = run_quality_eval(
            model, tokenizer, dataset, args.eval_samples
        )

        # 6. ç”Ÿæˆå ±å‘Š
        report_file = generate_report(
            param_info,
            perf_tracker,
            quality_results,
            predictions,
            references,
            args.adapter_path,
            adapter_loaded,
            args.output_dir
        )

        # å®Œæˆ
        print("=" * 80)
        print("ğŸ‰ è©•ä¼°å®Œæˆ!")
        print("=" * 80)
        print(f"\nçµæŸæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"å ±å‘Šæª”æ¡ˆ: {report_file}\n")

        return 0

    except KeyboardInterrupt:
        print("\n\nâš ï¸  è©•ä¼°è¢«ä¸­æ–·")
        return 1
    except Exception as e:
        print(f"\n\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
